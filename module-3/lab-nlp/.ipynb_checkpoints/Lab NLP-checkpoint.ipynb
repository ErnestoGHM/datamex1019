{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Ironhack is a Global Tech School ranked num 2 worldwide. Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do. This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course. We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack is a Global Tech School ranked num 2 worldwide.',\n",
       " 'Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.',\n",
       " 'This ideology is reflected in our teaching practices, which consist of a nine-weeks immersive programming, UX/UI design or Data Analytics course as well as a one-week hiring fair aimed at helping our students change their career and get a job straight after the course.',\n",
       " 'We are present in 8 countries and have campuses in 9 locations - Madrid, Barcelona, Miami, Paris, Mexico City,  Berlin, Amsterdam, Sao Paulo and Lisbon.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ironhack',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Global',\n",
       " 'Tech',\n",
       " 'School',\n",
       " 'ranked',\n",
       " 'num',\n",
       " '2',\n",
       " 'worldwide',\n",
       " '.',\n",
       " 'Our',\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'help',\n",
       " 'people',\n",
       " 'transform',\n",
       " 'their',\n",
       " 'careers',\n",
       " 'and',\n",
       " 'join',\n",
       " 'a',\n",
       " 'thriving',\n",
       " 'community',\n",
       " 'of',\n",
       " 'tech',\n",
       " 'professionals',\n",
       " 'that',\n",
       " 'love',\n",
       " 'what',\n",
       " 'they',\n",
       " 'do',\n",
       " '.',\n",
       " 'This',\n",
       " 'ideology',\n",
       " 'is',\n",
       " 'reflected',\n",
       " 'in',\n",
       " 'our',\n",
       " 'teaching',\n",
       " 'practices',\n",
       " ',',\n",
       " 'which',\n",
       " 'consist',\n",
       " 'of',\n",
       " 'a',\n",
       " 'nine-weeks',\n",
       " 'immersive',\n",
       " 'programming',\n",
       " ',',\n",
       " 'UX/UI',\n",
       " 'design',\n",
       " 'or',\n",
       " 'Data',\n",
       " 'Analytics',\n",
       " 'course',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'a',\n",
       " 'one-week',\n",
       " 'hiring',\n",
       " 'fair',\n",
       " 'aimed',\n",
       " 'at',\n",
       " 'helping',\n",
       " 'our',\n",
       " 'students',\n",
       " 'change',\n",
       " 'their',\n",
       " 'career',\n",
       " 'and',\n",
       " 'get',\n",
       " 'a',\n",
       " 'job',\n",
       " 'straight',\n",
       " 'after',\n",
       " 'the',\n",
       " 'course',\n",
       " '.',\n",
       " 'We',\n",
       " 'are',\n",
       " 'present',\n",
       " 'in',\n",
       " '8',\n",
       " 'countries',\n",
       " 'and',\n",
       " 'have',\n",
       " 'campuses',\n",
       " 'in',\n",
       " '9',\n",
       " 'locations',\n",
       " '-',\n",
       " 'Madrid',\n",
       " ',',\n",
       " 'Barcelona',\n",
       " ',',\n",
       " 'Miami',\n",
       " ',',\n",
       " 'Paris',\n",
       " ',',\n",
       " 'Mexico',\n",
       " 'City',\n",
       " ',',\n",
       " 'Berlin',\n",
       " ',',\n",
       " 'Amsterdam',\n",
       " ',',\n",
       " 'Sao',\n",
       " 'Paulo',\n",
       " 'and',\n",
       " 'Lisbon',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '''@Ironhack's-#Q website 776-is http://ironhack.com [(2018)]\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(s):\n",
    "    x = re.sub('\\d','',re.sub('\\W',' ',str(s.lower().split(' http:')[0])).strip())\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = clean_up(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = word_tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_and_lemmatize(lst):\n",
    "    x_stem_lemma = []\n",
    "    for i in range(len(lst)):\n",
    "        x_stem_lemma.append(lemma.lemmatize(stem.stem(lst[i])))\n",
    "        \n",
    "    return x_stem_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stem_and_lemmatize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lst):\n",
    "    x_nosw = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for i in lst:\n",
    "        if i not in stopWords:\n",
    "            x_nosw.append(i)\n",
    "    return x_nosw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ironhack', 'q', 'websit']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Ironhack is a Global Tech School ranked num 2 worldwide. Our mission is to help people transform their careers and join a thriving community of tech professionals that love what they do.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.741, 'pos': 0.259, 'compound': 0.8442}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf=zipfile.ZipFile('Sentiment140.csv.zip')\n",
    "chunks=pd.read_csv(zf.open('Sentiment140.csv'), chunksize = 200000)\n",
    "data = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_processed'] = data.text.apply(lambda x : remove_stopwords(stem_and_lemmatize(word_tokenize(clean_up(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[switchfoot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[upset, updat, hi, facebook, text, might, cri,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[kenichan, dive, mani, time, ball, manag, save...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[whole, bodi, feel, itchi, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[nationwideclass, behav, mad, whi, becaus, see]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \\\n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                      text_processed  \n",
       "0                                       [switchfoot]  \n",
       "1  [upset, updat, hi, facebook, text, might, cri,...  \n",
       "2  [kenichan, dive, mani, time, ball, manag, save...  \n",
       "3             [whole, bodi, feel, itchi, like, fire]  \n",
       "4    [nationwideclass, behav, mad, whi, becaus, see]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = []\n",
    "for t in range(len((data.text_processed))):\n",
    "    bag += data.text_processed[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for token in bag:\n",
    "    if token not in wordfreq.keys():\n",
    "        wordfreq[token] = 1\n",
    "    else:\n",
    "        wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545188"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = nltk.FreqDist(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545188"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq5000 = {}\n",
    "for k,v in allwords.items():\n",
    "    if v > 5000:\n",
    "        wordfreq5000[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'updat': 9207,\n",
       " 'hi': 24782,\n",
       " 'text': 5716,\n",
       " 'might': 9692,\n",
       " 'cri': 8655,\n",
       " 'school': 21050,\n",
       " 'today': 68567,\n",
       " 'also': 10348,\n",
       " 'mani': 8911,\n",
       " 'time': 66038,\n",
       " 'rest': 5898,\n",
       " 'go': 138211,\n",
       " 'whole': 5976,\n",
       " 'feel': 51128,\n",
       " 'like': 82963,\n",
       " 'whi': 28369,\n",
       " 'becaus': 13570,\n",
       " 'see': 50842,\n",
       " 'need': 43211,\n",
       " 'hug': 5205,\n",
       " 'hey': 19025,\n",
       " 'long': 17177,\n",
       " 'ye': 18914,\n",
       " 'rain': 17170,\n",
       " 'bit': 13113,\n",
       " 'onli': 27925,\n",
       " 'lol': 59028,\n",
       " 'thank': 58961,\n",
       " 'break': 6225,\n",
       " 'watch': 43513,\n",
       " 'thought': 12659,\n",
       " 'wa': 104501,\n",
       " 'either': 5336,\n",
       " 'never': 17857,\n",
       " 'talk': 14882,\n",
       " 'anymor': 6647,\n",
       " 'would': 27135,\n",
       " 'first': 16740,\n",
       " 'realli': 49707,\n",
       " 'though': 23834,\n",
       " 'wish': 33737,\n",
       " 'got': 70849,\n",
       " 'miss': 56968,\n",
       " 'hurt': 14119,\n",
       " 'alway': 14853,\n",
       " 'want': 56949,\n",
       " 'love': 81237,\n",
       " 'oh': 39644,\n",
       " 'drink': 7654,\n",
       " 'day': 108719,\n",
       " 'get': 110389,\n",
       " 'much': 36805,\n",
       " 'done': 15311,\n",
       " 'one': 56716,\n",
       " 'friend': 25937,\n",
       " 'call': 15560,\n",
       " 'ask': 7167,\n",
       " 'meet': 9437,\n",
       " 'thi': 93303,\n",
       " 'week': 28258,\n",
       " 'hope': 44597,\n",
       " 'class': 7620,\n",
       " 'tomorrow': 34288,\n",
       " 'hate': 22955,\n",
       " 'wake': 6648,\n",
       " 'peopl': 20896,\n",
       " 'sleep': 32422,\n",
       " 'im': 50595,\n",
       " 'sad': 29855,\n",
       " 'ok': 16196,\n",
       " 'almost': 7841,\n",
       " 'everi': 6992,\n",
       " 'make': 42901,\n",
       " 'new': 42186,\n",
       " 'may': 8283,\n",
       " 'b': 10532,\n",
       " 'morn': 35669,\n",
       " 'work': 87579,\n",
       " 'eye': 6147,\n",
       " 'night': 45684,\n",
       " 'sick': 16354,\n",
       " 'hour': 20995,\n",
       " 'sit': 7299,\n",
       " 'caus': 7572,\n",
       " 'back': 57161,\n",
       " 'bed': 22410,\n",
       " 'ill': 6879,\n",
       " 'tell': 12026,\n",
       " 'ya': 10871,\n",
       " 'later': 10352,\n",
       " 'good': 91997,\n",
       " 'sorri': 26261,\n",
       " 'came': 5986,\n",
       " 'think': 50869,\n",
       " 'even': 22713,\n",
       " 'know': 54622,\n",
       " 'kid': 9515,\n",
       " 'anoth': 14521,\n",
       " 'gon': 24021,\n",
       " 'na': 41926,\n",
       " 'studi': 8071,\n",
       " 'exam': 9853,\n",
       " 'ha': 39248,\n",
       " 'enough': 6777,\n",
       " 'heart': 5440,\n",
       " 'wan': 16659,\n",
       " 'still': 43389,\n",
       " 'awww': 5534,\n",
       " 'soo': 5166,\n",
       " 'final': 18956,\n",
       " 'fall': 5024,\n",
       " 'girl': 15684,\n",
       " 'found': 8653,\n",
       " 'famili': 8128,\n",
       " 'yay': 13722,\n",
       " 'happi': 27509,\n",
       " 'job': 9317,\n",
       " 'mean': 11825,\n",
       " 'check': 11326,\n",
       " 'look': 38570,\n",
       " 'happen': 10483,\n",
       " 'w': 10339,\n",
       " 'man': 14064,\n",
       " 'befor': 13026,\n",
       " 'plan': 8078,\n",
       " 'wow': 10928,\n",
       " 'repli': 6276,\n",
       " 'tweet': 22232,\n",
       " 'lot': 14482,\n",
       " 'take': 24603,\n",
       " 'put': 10001,\n",
       " 'photo': 5255,\n",
       " 'ago': 5386,\n",
       " 'name': 7060,\n",
       " 'sure': 15355,\n",
       " 'dont': 18159,\n",
       " 'away': 12267,\n",
       " 'show': 20620,\n",
       " 'ta': 9935,\n",
       " 'say': 25391,\n",
       " 'use': 18340,\n",
       " 'music': 9480,\n",
       " 'game': 12211,\n",
       " 'ugh': 10040,\n",
       " 'u': 69682,\n",
       " 'move': 8528,\n",
       " 'alreadi': 14689,\n",
       " 'glad': 10483,\n",
       " 'hear': 11025,\n",
       " 'well': 42760,\n",
       " 'p': 9270,\n",
       " 'play': 19585,\n",
       " 'leav': 14101,\n",
       " 'life': 16294,\n",
       " 'cool': 14394,\n",
       " 'post': 9175,\n",
       " 'nice': 23613,\n",
       " 'bad': 27216,\n",
       " 'come': 38703,\n",
       " 'around': 9049,\n",
       " 'lost': 11439,\n",
       " 'pay': 5213,\n",
       " 'phone': 14311,\n",
       " 'aw': 6073,\n",
       " 'money': 6376,\n",
       " 'hell': 5661,\n",
       " 'n': 12048,\n",
       " 'soon': 17725,\n",
       " 'saw': 9870,\n",
       " 'haha': 31204,\n",
       " 'someon': 11700,\n",
       " 'right': 27935,\n",
       " 'start': 21725,\n",
       " 'least': 8002,\n",
       " 'busi': 8417,\n",
       " 'run': 9846,\n",
       " 'ani': 15421,\n",
       " 'cute': 7260,\n",
       " 'tonight': 26130,\n",
       " 'wonder': 10107,\n",
       " 'ur': 13106,\n",
       " 'book': 8549,\n",
       " 'mine': 7224,\n",
       " 'boo': 5306,\n",
       " 'damn': 11934,\n",
       " 'anyth': 8123,\n",
       " 'stop': 10411,\n",
       " 'poor': 8108,\n",
       " 'plea': 16733,\n",
       " 'babi': 11441,\n",
       " 'birthday': 10524,\n",
       " 'parti': 11131,\n",
       " 'headach': 6169,\n",
       " 'enjoy': 13481,\n",
       " 'problem': 5444,\n",
       " 'thing': 26377,\n",
       " 'find': 15705,\n",
       " 'littl': 16820,\n",
       " 'sinc': 9433,\n",
       " 'abl': 5457,\n",
       " 'actual': 10550,\n",
       " 'follow': 27832,\n",
       " 'yesterday': 9617,\n",
       " 'pain': 5909,\n",
       " 'fail': 6032,\n",
       " 'quot': 72817,\n",
       " 'hous': 13388,\n",
       " 'rememb': 6124,\n",
       " 'kind': 5237,\n",
       " 'train': 5994,\n",
       " 'send': 6868,\n",
       " 'year': 16655,\n",
       " 'sooo': 6166,\n",
       " 'kill': 5874,\n",
       " 'believ': 7652,\n",
       " 'cuz': 5132,\n",
       " 'yeah': 22400,\n",
       " 'cant': 17584,\n",
       " 'hang': 5976,\n",
       " 'suck': 15645,\n",
       " 'aww': 8314,\n",
       " 'beach': 6004,\n",
       " 'head': 15195,\n",
       " 'sound': 12785,\n",
       " 'made': 13677,\n",
       " 'late': 11098,\n",
       " 'c': 6403,\n",
       " 'big': 11099,\n",
       " 'fan': 6420,\n",
       " 'must': 8591,\n",
       " 'el': 6131,\n",
       " 'wait': 32356,\n",
       " 'till': 7915,\n",
       " 'someth': 14383,\n",
       " 'seem': 8879,\n",
       " 'cold': 8783,\n",
       " 'weather': 10372,\n",
       " 'turn': 7307,\n",
       " 'stuff': 10060,\n",
       " 'said': 9554,\n",
       " 'tire': 17357,\n",
       " 'tri': 28137,\n",
       " 'let': 18477,\n",
       " 'th': 9770,\n",
       " 'test': 5005,\n",
       " 'win': 7669,\n",
       " 'without': 7196,\n",
       " 'guess': 12136,\n",
       " 'finish': 12239,\n",
       " 'probabl': 6573,\n",
       " 'left': 11440,\n",
       " 'read': 14028,\n",
       " 'amp': 48333,\n",
       " 'end': 11132,\n",
       " 'die': 8480,\n",
       " 'mom': 10103,\n",
       " 'worri': 5575,\n",
       " 'better': 23116,\n",
       " 'keep': 14686,\n",
       " 'live': 14366,\n",
       " 'fun': 28468,\n",
       " 'food': 7957,\n",
       " 'free': 7681,\n",
       " 'hard': 9693,\n",
       " 'hot': 11001,\n",
       " 'sweet': 7563,\n",
       " 'pretti': 12439,\n",
       " 'way': 25084,\n",
       " 'alon': 5085,\n",
       " 'kinda': 5930,\n",
       " 'far': 7661,\n",
       " 'crazi': 5900,\n",
       " 'full': 5310,\n",
       " 'song': 13342,\n",
       " 'stay': 9744,\n",
       " 'car': 9642,\n",
       " 'trip': 5440,\n",
       " 'close': 5839,\n",
       " 'woke': 8026,\n",
       " 'e': 5435,\n",
       " 'earli': 11826,\n",
       " 'yet': 13298,\n",
       " 'bore': 15292,\n",
       " 'beauti': 8990,\n",
       " 'guy': 19413,\n",
       " 'next': 18584,\n",
       " 'dog': 6580,\n",
       " 'shit': 7550,\n",
       " 'luck': 8652,\n",
       " 'help': 15162,\n",
       " 'seen': 5934,\n",
       " 'hahaha': 6858,\n",
       " 'mind': 5330,\n",
       " 'world': 9599,\n",
       " 'shop': 8130,\n",
       " 'movi': 16539,\n",
       " 'pictur': 7424,\n",
       " 'ever': 12778,\n",
       " 'tho': 7828,\n",
       " 'total': 7920,\n",
       " 'forgot': 5457,\n",
       " 'amaz': 12000,\n",
       " 'twitter': 36020,\n",
       " 'stupid': 6907,\n",
       " 'outsid': 6918,\n",
       " 'veri': 26507,\n",
       " 'news': 5591,\n",
       " 'forward': 7314,\n",
       " 'open': 5921,\n",
       " 'home': 40765,\n",
       " 'old': 12115,\n",
       " 'walk': 7202,\n",
       " 'hello': 5638,\n",
       " 'eat': 14312,\n",
       " 'place': 7572,\n",
       " 'lt': 18304,\n",
       " 'offic': 5390,\n",
       " 'weekend': 19586,\n",
       " 'noth': 11051,\n",
       " 'write': 7007,\n",
       " 'gt': 7376,\n",
       " 'video': 8902,\n",
       " 'iphon': 7852,\n",
       " 'chang': 7579,\n",
       " 'internet': 5079,\n",
       " 'fuck': 9755,\n",
       " 'face': 6161,\n",
       " 'excit': 12392,\n",
       " 'link': 5148,\n",
       " 'went': 13259,\n",
       " 'gone': 8315,\n",
       " 'r': 8976,\n",
       " 'danc': 5229,\n",
       " 'ah': 5224,\n",
       " 'doe': 10274,\n",
       " 'http': 12384,\n",
       " 'twitpic': 9778,\n",
       " 'com': 18950,\n",
       " 'two': 10226,\n",
       " 'idea': 6944,\n",
       " 'awesom': 18396,\n",
       " 'sun': 10423,\n",
       " 'omg': 12057,\n",
       " 'last': 36149,\n",
       " 'hair': 7852,\n",
       " 'mayb': 12626,\n",
       " 'word': 5692,\n",
       " 'half': 5695,\n",
       " 'listen': 11774,\n",
       " 'pm': 5608,\n",
       " 'www': 5333,\n",
       " 'dad': 6958,\n",
       " 'monday': 9070,\n",
       " 'onc': 6403,\n",
       " 'funni': 7474,\n",
       " 'drive': 7392,\n",
       " 'anyon': 7412,\n",
       " 'could': 21758,\n",
       " 'anyway': 5596,\n",
       " 'everyon': 16722,\n",
       " 'saturday': 6516,\n",
       " 'okay': 7956,\n",
       " 'everyth': 7580,\n",
       " 'dream': 7026,\n",
       " 'sister': 5171,\n",
       " 'god': 9730,\n",
       " 'buy': 8129,\n",
       " 'person': 6244,\n",
       " 'real': 6864,\n",
       " 'best': 16322,\n",
       " 'brother': 6094,\n",
       " 'rock': 6048,\n",
       " 'took': 5667,\n",
       " 'readi': 13979,\n",
       " 'comput': 5753,\n",
       " 'friday': 9132,\n",
       " 'true': 5556,\n",
       " 'goodnight': 5535,\n",
       " 'minut': 6363,\n",
       " 'care': 5704,\n",
       " 'hit': 6138,\n",
       " 'room': 6722,\n",
       " 'pic': 10178,\n",
       " 'month': 7776,\n",
       " 'great': 33241,\n",
       " 'part': 5433,\n",
       " 'give': 11384,\n",
       " 'wrong': 6665,\n",
       " 'boy': 8345,\n",
       " 'clean': 6948,\n",
       " 'sunday': 9197,\n",
       " 'coffe': 6861,\n",
       " 'quit': 5659,\n",
       " 'bring': 5373,\n",
       " 'didnt': 6589,\n",
       " 'summer': 12251,\n",
       " 'tv': 6423,\n",
       " 'mother': 5919,\n",
       " 'x': 15348,\n",
       " 'dinner': 7663,\n",
       " 'hehe': 5012,\n",
       " 'super': 5793,\n",
       " 'blog': 5947,\n",
       " 'lunch': 7873,\n",
       " 'xx': 6114,\n",
       " 'welcom': 7522}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['updat',\n",
       " 'hi',\n",
       " 'text',\n",
       " 'might',\n",
       " 'cri',\n",
       " 'school',\n",
       " 'today',\n",
       " 'also',\n",
       " 'mani',\n",
       " 'time',\n",
       " 'rest',\n",
       " 'go',\n",
       " 'whole',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'whi',\n",
       " 'becaus',\n",
       " 'see',\n",
       " 'need',\n",
       " 'hug',\n",
       " 'hey',\n",
       " 'long',\n",
       " 'ye',\n",
       " 'rain',\n",
       " 'bit',\n",
       " 'onli',\n",
       " 'lol',\n",
       " 'thank',\n",
       " 'break',\n",
       " 'watch',\n",
       " 'thought',\n",
       " 'wa',\n",
       " 'either',\n",
       " 'never',\n",
       " 'talk',\n",
       " 'anymor',\n",
       " 'would',\n",
       " 'first',\n",
       " 'realli',\n",
       " 'though',\n",
       " 'wish',\n",
       " 'got',\n",
       " 'miss',\n",
       " 'hurt',\n",
       " 'alway',\n",
       " 'want',\n",
       " 'love',\n",
       " 'oh',\n",
       " 'drink',\n",
       " 'day',\n",
       " 'get',\n",
       " 'much',\n",
       " 'done',\n",
       " 'one',\n",
       " 'friend',\n",
       " 'call',\n",
       " 'ask',\n",
       " 'meet',\n",
       " 'thi',\n",
       " 'week',\n",
       " 'hope',\n",
       " 'class',\n",
       " 'tomorrow',\n",
       " 'hate',\n",
       " 'wake',\n",
       " 'peopl',\n",
       " 'sleep',\n",
       " 'im',\n",
       " 'sad',\n",
       " 'ok',\n",
       " 'almost',\n",
       " 'everi',\n",
       " 'make',\n",
       " 'new',\n",
       " 'may',\n",
       " 'b',\n",
       " 'morn',\n",
       " 'work',\n",
       " 'eye',\n",
       " 'night',\n",
       " 'sick',\n",
       " 'hour',\n",
       " 'sit',\n",
       " 'caus',\n",
       " 'back',\n",
       " 'bed',\n",
       " 'ill',\n",
       " 'tell',\n",
       " 'ya',\n",
       " 'later',\n",
       " 'good',\n",
       " 'sorri',\n",
       " 'came',\n",
       " 'think',\n",
       " 'even',\n",
       " 'know',\n",
       " 'kid',\n",
       " 'anoth',\n",
       " 'gon',\n",
       " 'na',\n",
       " 'studi',\n",
       " 'exam',\n",
       " 'ha',\n",
       " 'enough',\n",
       " 'heart',\n",
       " 'wan',\n",
       " 'still',\n",
       " 'awww',\n",
       " 'soo',\n",
       " 'final',\n",
       " 'fall',\n",
       " 'girl',\n",
       " 'found',\n",
       " 'famili',\n",
       " 'yay',\n",
       " 'happi',\n",
       " 'job',\n",
       " 'mean',\n",
       " 'check',\n",
       " 'look',\n",
       " 'happen',\n",
       " 'w',\n",
       " 'man',\n",
       " 'befor',\n",
       " 'plan',\n",
       " 'wow',\n",
       " 'repli',\n",
       " 'tweet',\n",
       " 'lot',\n",
       " 'take',\n",
       " 'put',\n",
       " 'photo',\n",
       " 'ago',\n",
       " 'name',\n",
       " 'sure',\n",
       " 'dont',\n",
       " 'away',\n",
       " 'show',\n",
       " 'ta',\n",
       " 'say',\n",
       " 'use',\n",
       " 'music',\n",
       " 'game',\n",
       " 'ugh',\n",
       " 'u',\n",
       " 'move',\n",
       " 'alreadi',\n",
       " 'glad',\n",
       " 'hear',\n",
       " 'well',\n",
       " 'p',\n",
       " 'play',\n",
       " 'leav',\n",
       " 'life',\n",
       " 'cool',\n",
       " 'post',\n",
       " 'nice',\n",
       " 'bad',\n",
       " 'come',\n",
       " 'around',\n",
       " 'lost',\n",
       " 'pay',\n",
       " 'phone',\n",
       " 'aw',\n",
       " 'money',\n",
       " 'hell',\n",
       " 'n',\n",
       " 'soon',\n",
       " 'saw',\n",
       " 'haha',\n",
       " 'someon',\n",
       " 'right',\n",
       " 'start',\n",
       " 'least',\n",
       " 'busi',\n",
       " 'run',\n",
       " 'ani',\n",
       " 'cute',\n",
       " 'tonight',\n",
       " 'wonder',\n",
       " 'ur',\n",
       " 'book',\n",
       " 'mine',\n",
       " 'boo',\n",
       " 'damn',\n",
       " 'anyth',\n",
       " 'stop',\n",
       " 'poor',\n",
       " 'plea',\n",
       " 'babi',\n",
       " 'birthday',\n",
       " 'parti',\n",
       " 'headach',\n",
       " 'enjoy',\n",
       " 'problem',\n",
       " 'thing',\n",
       " 'find',\n",
       " 'littl',\n",
       " 'sinc',\n",
       " 'abl',\n",
       " 'actual',\n",
       " 'follow',\n",
       " 'yesterday',\n",
       " 'pain',\n",
       " 'fail',\n",
       " 'quot',\n",
       " 'hous',\n",
       " 'rememb',\n",
       " 'kind',\n",
       " 'train',\n",
       " 'send',\n",
       " 'year',\n",
       " 'sooo',\n",
       " 'kill',\n",
       " 'believ',\n",
       " 'cuz',\n",
       " 'yeah',\n",
       " 'cant',\n",
       " 'hang',\n",
       " 'suck',\n",
       " 'aww',\n",
       " 'beach',\n",
       " 'head',\n",
       " 'sound',\n",
       " 'made',\n",
       " 'late',\n",
       " 'c',\n",
       " 'big',\n",
       " 'fan',\n",
       " 'must',\n",
       " 'el',\n",
       " 'wait',\n",
       " 'till',\n",
       " 'someth',\n",
       " 'seem',\n",
       " 'cold',\n",
       " 'weather',\n",
       " 'turn',\n",
       " 'stuff',\n",
       " 'said',\n",
       " 'tire',\n",
       " 'tri',\n",
       " 'let',\n",
       " 'th',\n",
       " 'test',\n",
       " 'win',\n",
       " 'without',\n",
       " 'guess',\n",
       " 'finish',\n",
       " 'probabl',\n",
       " 'left',\n",
       " 'read',\n",
       " 'amp',\n",
       " 'end',\n",
       " 'die',\n",
       " 'mom',\n",
       " 'worri',\n",
       " 'better',\n",
       " 'keep',\n",
       " 'live',\n",
       " 'fun',\n",
       " 'food',\n",
       " 'free',\n",
       " 'hard',\n",
       " 'hot',\n",
       " 'sweet',\n",
       " 'pretti',\n",
       " 'way',\n",
       " 'alon',\n",
       " 'kinda',\n",
       " 'far',\n",
       " 'crazi',\n",
       " 'full',\n",
       " 'song',\n",
       " 'stay',\n",
       " 'car',\n",
       " 'trip',\n",
       " 'close',\n",
       " 'woke',\n",
       " 'e',\n",
       " 'earli',\n",
       " 'yet',\n",
       " 'bore',\n",
       " 'beauti',\n",
       " 'guy',\n",
       " 'next',\n",
       " 'dog',\n",
       " 'shit',\n",
       " 'luck',\n",
       " 'help',\n",
       " 'seen',\n",
       " 'hahaha',\n",
       " 'mind',\n",
       " 'world',\n",
       " 'shop',\n",
       " 'movi',\n",
       " 'pictur',\n",
       " 'ever',\n",
       " 'tho',\n",
       " 'total',\n",
       " 'forgot',\n",
       " 'amaz',\n",
       " 'twitter',\n",
       " 'stupid',\n",
       " 'outsid',\n",
       " 'veri',\n",
       " 'news',\n",
       " 'forward',\n",
       " 'open',\n",
       " 'home',\n",
       " 'old',\n",
       " 'walk',\n",
       " 'hello',\n",
       " 'eat',\n",
       " 'place',\n",
       " 'lt',\n",
       " 'offic',\n",
       " 'weekend',\n",
       " 'noth',\n",
       " 'write',\n",
       " 'gt',\n",
       " 'video',\n",
       " 'iphon',\n",
       " 'chang',\n",
       " 'internet',\n",
       " 'fuck',\n",
       " 'face',\n",
       " 'excit',\n",
       " 'link',\n",
       " 'went',\n",
       " 'gone',\n",
       " 'r',\n",
       " 'danc',\n",
       " 'ah',\n",
       " 'doe',\n",
       " 'http',\n",
       " 'twitpic',\n",
       " 'com',\n",
       " 'two',\n",
       " 'idea',\n",
       " 'awesom',\n",
       " 'sun',\n",
       " 'omg',\n",
       " 'last',\n",
       " 'hair',\n",
       " 'mayb',\n",
       " 'word',\n",
       " 'half',\n",
       " 'listen',\n",
       " 'pm',\n",
       " 'www',\n",
       " 'dad',\n",
       " 'monday',\n",
       " 'onc',\n",
       " 'funni',\n",
       " 'drive',\n",
       " 'anyon',\n",
       " 'could',\n",
       " 'anyway',\n",
       " 'everyon',\n",
       " 'saturday',\n",
       " 'okay',\n",
       " 'everyth',\n",
       " 'dream',\n",
       " 'sister',\n",
       " 'god',\n",
       " 'buy',\n",
       " 'person',\n",
       " 'real',\n",
       " 'best',\n",
       " 'brother',\n",
       " 'rock',\n",
       " 'took',\n",
       " 'readi',\n",
       " 'comput',\n",
       " 'friday',\n",
       " 'true',\n",
       " 'goodnight',\n",
       " 'minut',\n",
       " 'care',\n",
       " 'hit',\n",
       " 'room',\n",
       " 'pic',\n",
       " 'month',\n",
       " 'great',\n",
       " 'part',\n",
       " 'give',\n",
       " 'wrong',\n",
       " 'boy',\n",
       " 'clean',\n",
       " 'sunday',\n",
       " 'coffe',\n",
       " 'quit',\n",
       " 'bring',\n",
       " 'didnt',\n",
       " 'summer',\n",
       " 'tv',\n",
       " 'mother',\n",
       " 'x',\n",
       " 'dinner',\n",
       " 'hehe',\n",
       " 'super',\n",
       " 'blog',\n",
       " 'lunch',\n",
       " 'xx',\n",
       " 'welcom']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = list(wordfreq5000.keys())\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(min_df=0.30, tokenizer=word_token)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(col):\n",
    "    features = {}\n",
    "    x = set(col)\n",
    "    for i in word_token:\n",
    "        features[i] = (i in x)    \n",
    "    z = analyzer.polarity_scores(\" \".join(col))\n",
    "    if z['pos'] > 0.5:\n",
    "        s = True\n",
    "    else:\n",
    "        s = False\n",
    "        \n",
    "    return (features, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "featmax = data.text_processed.apply(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featmax[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = featmax[:800000]\n",
    "test_set = featmax[800000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86354375"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
